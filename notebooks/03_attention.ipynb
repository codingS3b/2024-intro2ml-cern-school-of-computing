{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73efddcf",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Exploring a reweighting task\n",
    "\n",
    "The following material is taken from the excellent lectures of Francois Fleuret (Univeristy of Geneva). Francois Fleuret introduces the self-attention mechanism in three parts: [first](https://fleuret.org/dlc/streaming/dlc-video-13-1-attention-memory-translation.mp4), [second](https://fleuret.org/dlc/streaming/dlc-video-13-2-attention-mechanisms.mp4) and [third](https://fleuret.org/dlc/streaming/dlc-video-13-3-transformers.mp4) - all of which are worthwhile watching. I have asked permission of Francois to reuse some of his material.\n",
    "\n",
    "## A regression task\n",
    "\n",
    "In the following, we again look at a regression task. The functions below produce a dataset which can be used to illustrate the use of the attention mechanism. The dataset exhibits 2 triangles and 2 boxes/rectangles on a 1D line. In this notebook, we focus on\n",
    "\n",
    "1. exploring the data set with a standard convnet.\n",
    "2. preparing a network using the attention mechanism\n",
    "3. compare the performance of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b7170",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from fleuret_data import generate_sequences\n",
    "\n",
    "drng = np.random.default_rng(43)  # set the RNG seed for reproducible runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test set\n",
    "train_input, train_targets, train_tr, train_bx = generate_sequences(\n",
    "    15000, seq_length=64, rng=drng\n",
    ")\n",
    "test_input, test_targets, test_tr, test_bx = generate_sequences(\n",
    "    1000, seq_length=64, rng=drng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape, test_targets.shape, test_tr.shape, test_bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ffcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "ax[0].plot(\n",
    "    np.arange(test_input[0].shape[-1]) + 0.5,\n",
    "    test_input[0].squeeze(),\n",
    "    color=\"blue\",\n",
    "    label=\"input\",\n",
    ")\n",
    "ax[0].set_title(\"input\")\n",
    "ax[1].plot(\n",
    "    np.arange(test_targets[0].shape[-1]) + 0.5,\n",
    "    test_targets[0].squeeze(),\n",
    "    color=\"red\",\n",
    "    label=\"target\",\n",
    ")\n",
    "ax[1].set_title(\"target\")\n",
    "fig.savefig(\"attention_dataset.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f9a99",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You see two kinds of \"objects\" in the signal plotted above: two box-like structures and two triangle-like structure. We now define a **regression task** which is meant to equalize the height of the boxes (new height should be the average height of the two input boxes) and the height of the triangles (new height of the triangles should be the mean of the two input triangles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec9271",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Convolutional Network\n",
    "\n",
    "First, we need to normalize the data into a dynamic range as $\\vec{x} \\in [0,1]$. Then, we like to create a regression model using convolutions only, which tries to accomplish the task above.\n",
    "\n",
    "### Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a824d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# set the seeds to make the notebook reproducible\n",
    "np.random.seed(41)\n",
    "torch.random.manual_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06749e06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# normalize the signal, zscale if required\n",
    "x_min, x_max = train_input.min(), train_input.max()\n",
    "x_ = (train_input - x_min) / (x_max - x_min)\n",
    "\n",
    "y_min, y_max = train_targets.min(), train_targets.max()\n",
    "y_ = (train_targets - y_min) / (y_max - y_min)\n",
    "\n",
    "x_test_ = (test_input - x_min) / (x_max - x_min)\n",
    "y_test_ = (test_targets - y_min) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"data shape check: {x_.shape, y_.shape, x_test_.shape, y_test_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28d3b7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### A fully convolutional model\n",
    "\n",
    "The term fully convolutional describes an architecture which consists exclusively of convolutional operations. This has the benefit as the model design is independent of the input data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionFCN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inshape, num_channels=64, ksize=5, num_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        num_inchannels = inshape[0]\n",
    "        padding = ksize // 2\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(\n",
    "                torch.nn.Conv1d(num_inchannels, num_channels, ksize,stride=1,padding=padding )\n",
    "            )\n",
    "            self.layers.append(\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "            num_inchannels = num_channels\n",
    "        self.layers.append(torch.nn.Conv1d(num_channels, inshape[0], 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83dae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "train_ds = torch.utils.data.StackDataset(x_, y_)\n",
    "test_ds = torch.utils.data.StackDataset(x_test_, y_test_)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33822986",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# test our model\n",
    "\n",
    "plainfcn = RegressionFCN(x_.shape[-2:])\n",
    "print(plainfcn)\n",
    "first_x, first_y = next(iter(train_loader))\n",
    "output = plainfcn(first_x)\n",
    "assert output.shape == first_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform training\n",
    "def train_regression(model, opt, crit, train_dataloader, test_dataloader, max_epochs, log_every=5):\n",
    "\n",
    "    results = {\"train_losses\": [], \"test_losses\": []}\n",
    "    ntrainsteps = len(train_dataloader)\n",
    "    nteststeps = len(test_dataloader)\n",
    "    train_loss, test_loss = torch.zeros((ntrainsteps,)), torch.zeros((nteststeps,))\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        # perform training for one epoch\n",
    "        for idx, (X, y) in enumerate(train_dataloader):\n",
    "            # forward pass\n",
    "            y_hat = model(X)\n",
    "\n",
    "            # compute loss\n",
    "            loss = crit(y_hat, y)\n",
    "\n",
    "            # compute gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # apply weight update rule\n",
    "            opt.step()\n",
    "\n",
    "            # set gradients to 0\n",
    "            opt.zero_grad()\n",
    "\n",
    "            train_loss[idx] = loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (X_test, y_test) in enumerate(test_dataloader):\n",
    "                X_prime_test = model(X_test)\n",
    "                loss_ = crit(X_prime_test, X_test)\n",
    "                test_loss[idx] = loss_.item()\n",
    "\n",
    "            results[\"train_losses\"].append(train_loss.mean())\n",
    "            results[\"test_losses\"].append(test_loss.mean())\n",
    "\n",
    "            if epoch % log_every == 0 or (epoch + 1) == max_epochs:\n",
    "                print(\n",
    "                    f\"{epoch+1:02.0f}/{max_epochs} :: training loss {train_loss.mean():03.4f}; test loss {test_loss.mean():03.4f}\"\n",
    "                )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc38cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(plainfcn.parameters(), lr=5e-4)\n",
    "crit  = torch.nn.MSELoss()\n",
    "max_epochs = 10\n",
    "fcnresults = train_regression(plainfcn, optim, crit, train_loader, test_loader, max_epochs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806516f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_history(history, metrics=[\"train_losses\", \"test_losses\"], metric_label=\"metric\", draw_legend=True):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "\n",
    "    Args:\n",
    "        history (keras History object that is returned by model.fit())\n",
    "        metrics(str, list): Metric or a list of metrics to plot\n",
    "    \"\"\"\n",
    "\n",
    "    f, ax = plt.subplots(1,1)\n",
    "    for k in history.keys():\n",
    "        ax.plot(history[k], label=k)\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(metric_label)\n",
    "    if draw_legend:\n",
    "        ax.legend()\n",
    "\n",
    "    return f, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba56a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plot_history(fcnresults)\n",
    "f.savefig(\"attention_plainfcn_losses.svg\")\n",
    "\n",
    "\n",
    "# # %%\n",
    "# let's visualise some example predictions\n",
    "xaxis = np.arange(0, x_test_.shape[-1], 1)\n",
    "first5_x_test, first5_y_test = next(iter(test_loader))\n",
    "pred5 = plainfcn(first5_x_test[:5,...]) # predict only first 5 samples\n",
    "\n",
    "f, ax = plt.subplots(1,5, figsize=(10,2), sharex=True, sharey=True)\n",
    "\n",
    "for col in range(5):\n",
    "    labl = first5_y_test[col:col+1].detach().squeeze().numpy()\n",
    "    pred = pred5[col:col+1].detach().squeeze().numpy()\n",
    "    ax[col].plot(labl, color=\"green\", label=\"label\")\n",
    "    ax[col].plot(pred, color=\"red\", label=\"prediction\")\n",
    "    ax[col].set_title(f\"test set item {col}\")\n",
    "    ax[col].set_ylabel(\"intensity / a.u.\")\n",
    "    ax[col].set_xlabel(\"sample / a.u.\")\n",
    "    if col == 4:\n",
    "        ax[col].legend()\n",
    "\n",
    "f.savefig(\"attention_plainfcn_pred5.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99748c7e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# The above is not a great model, actually it doesn't work at all! But we expected no less as the test loss didn't decrease any further than `0.0085` while the training loss decreased further and further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2873f7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Your own Attention Layer\n",
    "\n",
    "Attention and self-attention are very powerful transformations. In this section, we will write our own Attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, key_channels\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # we want to establish queries Q, keys K and values V\n",
    "        # instead of using Linear layers, we opt for Conv1D as they use less\n",
    "        # parameters and hence are less memory intensive\n",
    "        self.conv_Q = torch.nn.Conv1d(in_channels,\n",
    "                                      key_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      bias=False\n",
    "                                      )\n",
    "        self.conv_K = torch.nn.Conv1d(in_channels,\n",
    "                                      key_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      bias=False\n",
    "                                      )\n",
    "        self.conv_V = torch.nn.Conv1d(in_channels,\n",
    "                                      out_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      bias=False\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run the convolutions on our inputs\n",
    "        Q = self.conv_Q(x)\n",
    "        K = self.conv_K(x)\n",
    "        V = self.conv_V(x)\n",
    "\n",
    "        # TODO: perform a tensor transpose\n",
    "        #       you want to transpose the very last dimension with the second to last\n",
    "        K_t = torch.transpose(K, -1, -2)\n",
    "\n",
    "        # TODO: perform a matrix multiplication of Q*K_t\n",
    "        A_ = torch.matmul(Q,K_t)\n",
    "\n",
    "        # TODO: perform a row-wise softmax of A_\n",
    "        A = torch.nn.functional.softmax(A_,dim=-2)\n",
    "\n",
    "        # TODO: perform a matrix multiplication of A*V\n",
    "        y = torch.matmul(A,V)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# # %%\n",
    "class CustomAttn(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inshape=x_test_.shape[-2:], num_channels=64, ksize=5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        num_inchannels = inshape[0]\n",
    "        padding = ksize // 2\n",
    "        self.layers.append(\n",
    "                torch.nn.Conv1d(num_inchannels, num_channels, ksize,stride=1,padding=padding )\n",
    "            )\n",
    "        self.layers.append(\n",
    "                torch.nn.ReLU()\n",
    "        )\n",
    "        self.layers.append(\n",
    "                SelfAttention(num_channels,num_channels,num_channels)\n",
    "        )\n",
    "        self.layers.append(\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layers.append(\n",
    "            torch.nn.Conv1d(num_channels, num_channels, ksize,stride=1,padding=padding )\n",
    "        )\n",
    "        self.layers.append(\n",
    "            torch.nn.Conv1d(num_channels, num_inchannels, 1,stride=1,padding=0 )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "attmodel = CustomAttn(x_test_.shape[1:])\n",
    "output = attmodel(first_x)\n",
    "assert output.shape == first_y.shape\n",
    "print(attmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baff841",
   "metadata": {},
   "outputs": [],
   "source": [
    "attoptim = torch.optim.AdamW(plainfcn.parameters(), lr=1e-3)\n",
    "attcrit  = torch.nn.MSELoss()\n",
    "max_epochs = 10\n",
    "attresults = train_regression(attmodel, attoptim, attcrit, train_loader, test_loader, max_epochs,2)\n",
    "\n",
    "# chistory = cmodel.fit(\n",
    "#     x, y, validation_data=(x_test, y_test), batch_size=128, epochs=10, verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c360930",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plot_history(attresults)\n",
    "f.savefig(\"attention_attmodel_losses.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685bdbc",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Create a model with attention\n",
    "\n",
    "The idea of attention was published in 2014 by A. Graves in \"Neural Turing Machines\", see [here](https://arxiv.org/abs/1410.5401)\n",
    "It was picked up again in 2017 by A. Vaswani et al in \"Attention is all you need\", see [here](https://arxiv.org/abs/1706.03762). This paper coined the term Transformer model which relies strongly on self-attention layers.\n",
    "A nice visualizer to help you grasp the idea of attention can be found [here](https://poloclub.github.io/transformer-explainer/).\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class CustomTorchAttn(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inshape=x_test_.shape[-2:], num_channels=64, ksize=5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_layers = torch.nn.Sequential()\n",
    "        num_inchannels = inshape[0]\n",
    "        padding = ksize // 2\n",
    "        self.head_layers.append(\n",
    "                torch.nn.Conv1d(num_inchannels, num_channels, ksize,stride=1,padding=padding )\n",
    "            )\n",
    "        self.head_layers.append(\n",
    "                torch.nn.ReLU()\n",
    "        )\n",
    "        self.head_layers.append(\n",
    "                torch.nn.Conv1d(num_channels, num_channels, ksize,stride=1,padding=padding )\n",
    "            )\n",
    "        self.head_layers.append(\n",
    "                torch.nn.ReLU()\n",
    "        )\n",
    "        self.attn_layer = torch.nn.MultiheadAttention(num_channels,\n",
    "                                        num_heads=1,\n",
    "                                        kdim=num_channels,\n",
    "                                                      vdim=num_channels)\n",
    "\n",
    "\n",
    "        self.tail_layers = torch.nn.Sequential()\n",
    "        self.tail_layers.append(\n",
    "            torch.nn.Conv1d(num_channels, num_channels, ksize,stride=1,padding=padding )\n",
    "        )\n",
    "        self.tail_layers.append(\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.tail_layers.append(\n",
    "            torch.nn.Conv1d(num_channels, num_inchannels, 1,stride=1,padding=0 )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        Q = self.head_layers(x)\n",
    "        K = self.head_layers(x)\n",
    "        V = self.head_layers(x)\n",
    "\n",
    "        embedded = self.attn_layer(Q,K,V, need_weights=False)\n",
    "        print(\">>\", [ it.shape for it in embedded ])\n",
    "        value = self.tail_layers(embedded)\n",
    "\n",
    "        return value\n",
    "\n",
    "tattmodel = CustomTorchAttn(x_test_.shape[1:])\n",
    "output = tattmodel(first_x)\n",
    "assert output.shape == first_y.shape\n",
    "print(tattmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tattoptim = torch.optim.AdamW(plainfcn.parameters(), lr=1e-3)\n",
    "tattcrit  = torch.nn.MSELoss()\n",
    "max_epochs = 10\n",
    "attresults = train_regression(tattmodel, tattoptim, tattcrit, train_loader, test_loader, max_epochs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plot_history(attresults)\n",
    "f.savefig(\"attention_tattmodel_losses.svg\")\n",
    "\n",
    "# # We reuse the model idea from above\n",
    "# def create_attn(inshape=x.shape[-2:], channels=64, ksize=5):\n",
    "#     \"a fully convolutional network (fcn) to regress the signal using selfattention from keras\"\n",
    "\n",
    "#     inputs = keras.layers.Input(shape=inshape)\n",
    "#     x = keras.layers.Conv1D(\n",
    "#         channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "#     )(inputs)\n",
    "#     x = keras.layers.Conv1D(\n",
    "#         channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "#     )(x)\n",
    "#     # TODO: Keras also has a built-in Attention Layer, find it\n",
    "#     #       and use in in similar fashion as our own custom attention above\n",
    "#     #       (note, we want to use one attention head)\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "#     x = keras.layers.Conv1D(\n",
    "#         channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "#     )(x)\n",
    "#     outputs = keras.layers.Conv1D(1, ksize, strides=1, padding=\"same\")(x)\n",
    "\n",
    "#     return keras.Model(\n",
    "#         inputs=inputs, outputs=outputs, name=\"fcn-regression-selfattention\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "# amodel = create_attn(x.shape[1:])\n",
    "# amodel.summary()  # a simple model\n",
    "\n",
    "# # %% [markdown]\n",
    "# # The keras built-in attention layer uses Linear layers internally. This gives rise to the large number of parameters in the multi-head attention layer above even though we only want to use 1 head.\n",
    "# #\n",
    "# # Let's compile the model and see the effect of this change.\n",
    "\n",
    "# # %%\n",
    "# amodel.compile(optimizer=\"adam\", loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "# # %%\n",
    "# ahistory = amodel.fit(\n",
    "#     x, y, validation_data=(x_test, y_test), batch_size=128, epochs=15, verbose=1\n",
    "# )\n",
    "\n",
    "\n",
    "# # %% [markdown]\n",
    "# # -\n",
    "\n",
    "# # %%\n",
    "# plot_histories(\n",
    "#     [history, ahistory, chistory],\n",
    "#     [\"loss\", \"val_loss\"],\n",
    "#     \"vanilla,self-attention, custom-attention\".split(\",\"),\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
